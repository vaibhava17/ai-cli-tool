# AI CLI Tool ðŸ¤–

**Smart command-line AI assistant with caching, multi-provider support, and context memory.**

Get instant responses from cached queries, or enhanced responses using RAG from similar past conversations. Automatically routes to the best AI provider for each task type.

---

## ðŸš€ Quick Start

### 1. Setup (5 minutes)
```bash
# Clone and setup
git clone <repository-url>
cd ai-cli-tool
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Start vector database
docker run -d -p 6333:6333 qdrant/qdrant

# Configure API keys
cp .env.example .env
# Edit .env with your API keys (need at least one):
# OPENAI_API_KEY=your_key
# CLAUDE_API_KEY=your_key  
# GEMINI_API_KEY=your_key
```

### 2. Basic Usage
```bash
# Always activate virtual environment first
source venv/bin/activate

# Ask any question - automatic provider selection
python main.py ask "What is machine learning?"

# Coding questions â†’ routes to coding provider
python main.py ask "Write a Python function to reverse a string"

# Analysis questions â†’ routes to reasoning provider  
python main.py ask "Compare Python vs JavaScript for web development"

# General conversation â†’ routes to general provider
python main.py ask "Hello! How are you today?"
```

---

## ðŸ’¡ Key Features in Action

### ðŸ”„ **Semantic Caching** - Same/similar questions get instant responses
```bash
# First time - calls AI provider
$ python main.py ask "What is artificial intelligence?"
ðŸ¤– AI RESPONSE
ðŸ”— Provider: gemini/gemini-1.5-flash | Category: general
ðŸ’° API Call: Made fresh request to gemini/gemini-1.5-flash

# Exact same question - instant cache hit  
$ python main.py ask "What is artificial intelligence?"
ðŸ”„ CACHE HIT (PERFECT) - Similarity: 1.000
ðŸ“¦ Original Source: gemini/gemini-1.5-flash | Category: general
âš¡ Response Time: Instant (0 API calls)
```

### ðŸ“š **RAG Context Enhancement** - Similar questions get enhanced with context
```bash
# Similar but different question gets RAG context
$ python main.py ask "Explain AI and machine learning differences"
ðŸ¤– AI RESPONSE + RAG Context
ðŸ”— Provider: openai/gpt-4o | Category: reasoning  
ðŸ’° API Call: Made fresh request to openai/gpt-4o
ðŸ“š Enhanced: Using context from previous interactions
```

### ðŸŽ¯ **Smart Provider Routing** - Right AI for each task type
```bash
# Coding â†’ Claude (if configured) or OpenAI
python main.py ask "Debug this Python code: def hello(): print('hi'"

# Analysis â†’ OpenAI GPT-4o
python main.py ask "Why is quantum computing important?"

# General chat â†’ Gemini (fast and cost-effective)
python main.py ask "Good morning! Nice weather today"
```

---

## ðŸ›  Advanced Usage

### Force Specific Provider
```bash
python main.py ask "Write a function" --provider openai
python main.py ask "Explain quantum physics" --provider gemini --model gemini-1.5-pro
```

### Process Files
```bash
python main.py ask "What's in this image?" --image photo.jpg
python main.py ask "Summarize this document" --file report.pdf
```

### Cache Control
```bash
python main.py ask "Current weather" --no-cache           # Skip cache
python main.py ask "Question" --threshold 0.95            # Higher similarity needed
```

### Management Commands
```bash
python main.py providers    # List available AI providers
python main.py stats        # Usage statistics
python main.py configure    # Check configuration
```

---

## ðŸ“Š Understanding the Output

### Cache Hit Example:
```
ðŸ”„ CACHE HIT (PERFECT) - Similarity: 1.000
ðŸ“¦ Original Source: openai/gpt-4o | Category: reasoning
âš¡ Response Time: Instant (0 API calls)
```

### AI Response Example:
```
ðŸ¤– AI RESPONSE + RAG Context  
ðŸ”— Provider: anthropic/claude-3-5-sonnet | Category: coding
ðŸ’° API Call: Made fresh request to anthropic/claude-3-5-sonnet
ðŸ“š Enhanced: Using context from previous interactions
```

### What This Tells You:
- **ðŸ”„/ðŸ¤–** = Cache hit vs Fresh AI response
- **Provider Used** = Which AI service answered
- **Category** = How the question was classified (reasoning/coding/general)
- **RAG Context** = Whether similar past conversations enhanced the response
- **Similarity Score** = How closely it matched cached responses (0-1)

---

## ðŸ”§ Configuration

### Essential Environment Variables
```bash
# At least one API key required:
OPENAI_API_KEY=your_openai_key
CLAUDE_API_KEY=your_claude_key  
GEMINI_API_KEY=your_gemini_key

# Vector database (auto-configured):
QDRANT_HOST=127.0.0.1
QDRANT_PORT=6333
```

### Customizing Provider Routing
```bash
# Override default provider assignments:
REASONING_PROVIDER=openai       # For analysis questions
CODING_PROVIDER=anthropic       # For programming questions  
GENERAL_PROVIDER=gemini         # For general conversation

# Specify models:
REASONING_MODEL=gpt-4o
CODING_MODEL=claude-3-5-sonnet-20241022
GENERAL_MODEL=gemini-1.5-flash
```

---

## ðŸŽ¯ Use Cases

### **Developer Workflow**
```bash
# Code help with context memory
python main.py ask "Write a Python REST API using FastAPI"
python main.py ask "Add authentication to that API"  # Uses previous context
python main.py ask "Write tests for the auth endpoints"  # Enhanced with context
```

### **Learning & Research**
```bash
# Build knowledge progressively  
python main.py ask "What is machine learning?"
python main.py ask "What are neural networks?"
python main.py ask "How do neural networks relate to machine learning?"  # Gets context
```

### **Instant Answers**
```bash
# Frequently asked questions get cached
python main.py ask "How to install Python packages?"  # First time: calls AI
python main.py ask "How to install Python packages?"  # Second time: instant cache
```

---

## âš¡ Performance Benefits

- **âš¡ Instant Responses** - Cached answers in milliseconds
- **ðŸ’° Cost Savings** - Avoid duplicate API calls  
- **ðŸ§  Context Memory** - Enhanced responses using conversation history
- **ðŸŽ¯ Smart Routing** - Best AI for each question type
- **ðŸ“ˆ Usage Analytics** - Track your AI usage patterns

---

## ðŸš¨ Troubleshooting

### Quick Fixes
```bash
# Qdrant not running?
docker run -d -p 6333:6333 qdrant/qdrant

# Missing dependencies?
source venv/bin/activate
pip install -r requirements.txt

# API key issues?
python main.py configure  # Check configuration status

# Clear cache?
rm -rf qdrant_storage/    # Reset all cached data
```

### Getting Help
```bash
python main.py --help              # Main help
python main.py ask --help          # Ask command options
python main.py stats               # Usage statistics
```

---

## ðŸ“š More Information

- **[Complete Documentation](main.md)** - Comprehensive guide with all features
- **[Architecture Details](main.md#architecture-details)** - How it works under the hood
- **[Development Guide](main.md#development)** - Contributing and extending
- **[Docker Deployment](main.md#docker-deployment)** - Production setup

---

## ðŸŒŸ Why Use This Tool?

âœ… **Saves Time** - Instant answers for repeated questions  
âœ… **Saves Money** - Cached responses reduce API costs  
âœ… **Context Aware** - Remembers and uses conversation history  
âœ… **Multi-Provider** - Best AI for each task automatically  
âœ… **Developer Friendly** - CLI tool that integrates into workflows  
âœ… **Open Source** - Fully customizable and extensible  

**Get started in 5 minutes and experience AI with memory! ðŸš€**